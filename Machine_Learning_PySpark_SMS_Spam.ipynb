{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine_Learning_PySpark_SMS_Spam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOWvL0B5BIWGlmr7UqqKcOc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Himagination/PySpark_Projects/blob/main/Machine_Learning_PySpark_SMS_Spam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dlvF-xQ8EeH"
      },
      "source": [
        "# Setting PySpark Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXrGciOS7xbK"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RatA8ujP8QTm"
      },
      "source": [
        "# Objective\n",
        "- Build a SMS Spam Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_rvYzEj8cjf"
      },
      "source": [
        "# The Data\n",
        "\n",
        "*  **SMS Spam Dataset**\n",
        "\n",
        "*  Notes on CSV format:\n",
        "\n",
        "No header record and\n",
        "fields are separated by a semicolon (this is not the default separator).\n",
        "\n",
        "*  Data dictionary:\n",
        "\n",
        "1.  id — record identifier\n",
        "2.  text — content of SMS message\n",
        "3.  label — spam or ham (integer; 0 = ham and 1 = spam)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCSN9Jlg8sL-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee3d7bdb-9455-438b-c6eb-a393b61da419"
      },
      "source": [
        "!wget https://assets.datacamp.com/production/repositories/3918/datasets/9da06fbd2f1decf3b91e2dbcb08eb0c44aa4d18e/sms.csv\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "# Specify column names and types\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType()),\n",
        "    StructField(\"text\", StringType()),\n",
        "    StructField(\"label\", IntegerType())\n",
        "])\n",
        "# Load data from a delimited file\n",
        "sms = spark.read.csv('sms.csv', sep=';', header=False, schema=schema)\n",
        "print(\"The data contain %d records.\" % sms.count())\n",
        "# View the first five records\n",
        "sms.show(5)\n",
        "# Print schema of DataFrame\n",
        "sms.printSchema()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-21 04:18:01--  https://assets.datacamp.com/production/repositories/3918/datasets/9da06fbd2f1decf3b91e2dbcb08eb0c44aa4d18e/sms.csv\n",
            "Resolving assets.datacamp.com (assets.datacamp.com)... 104.18.16.147, 104.18.17.147\n",
            "Connecting to assets.datacamp.com (assets.datacamp.com)|104.18.16.147|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 488033 (477K) [text/csv]\n",
            "Saving to: ‘sms.csv’\n",
            "\n",
            "sms.csv             100%[===================>] 476.59K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-10-21 04:18:01 (15.5 MB/s) - ‘sms.csv’ saved [488033/488033]\n",
            "\n",
            "The data contain 5574 records.\n",
            "+---+--------------------+-----+\n",
            "| id|                text|label|\n",
            "+---+--------------------+-----+\n",
            "|  1|Sorry, I'll call ...|    0|\n",
            "|  2|Dont worry. I gue...|    0|\n",
            "|  3|Call FREEPHONE 08...|    1|\n",
            "|  4|Win a 1000 cash p...|    1|\n",
            "|  5|Go until jurong p...|    0|\n",
            "+---+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- label: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIMLeOHy9CEm"
      },
      "source": [
        "# Data Preparation\n",
        "\n",
        "1. Remove punctuation and numbers\n",
        "2. Tokenize (split into individual words)\n",
        "3. Remove stop words\n",
        "4. Apply the hashing trick\n",
        "- The hashing trick provides a fast and space-efficient way to map a very large (possibly infinite) set of items (in this case, all words contained in the SMS messages) onto a smaller, finite number of values.\n",
        "5. Convert to TF-IDF representation.\n",
        "- The TF-IDF matrix reflects how important a word is to each document. It takes into account both the frequency of the word within each document but also the frequency of the word across all of the documents in the collection\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3jvQo7uJsOg",
        "outputId": "6fbaf55b-733c-4a06-acbd-3cfe698a7ce5"
      },
      "source": [
        "# Import the necessary functions\n",
        "from pyspark.sql.functions import regexp_replace\n",
        "from pyspark.ml.feature import Tokenizer\n",
        "\n",
        "# Remove punctuation (REGEX provided) and numbers\n",
        "wrangled = sms.withColumn('text', regexp_replace(sms.text, '[_():;,.!?\\\\-]', ' '))\n",
        "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, '[0-9]', ' '))\n",
        "\n",
        "# Merge multiple spaces\n",
        "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, ' +', ' '))\n",
        "\n",
        "# Split the text into words\n",
        "wrangled = Tokenizer(inputCol='text', outputCol='words').transform(wrangled)\n",
        "\n",
        "wrangled.show(4, truncate=False)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------------------------+-----+------------------------------------------+\n",
            "|id |text                              |label|words                                     |\n",
            "+---+----------------------------------+-----+------------------------------------------+\n",
            "|1  |Sorry I'll call later in meeting  |0    |[sorry, i'll, call, later, in, meeting]   |\n",
            "|2  |Dont worry I guess he's busy      |0    |[dont, worry, i, guess, he's, busy]       |\n",
            "|3  |Call FREEPHONE now                |1    |[call, freephone, now]                    |\n",
            "|4  |Win a cash prize or a prize worth |1    |[win, a, cash, prize, or, a, prize, worth]|\n",
            "+---+----------------------------------+-----+------------------------------------------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMeldRZAQD1B",
        "outputId": "324c45bd-0fbb-4fa9-cdac-e5f66eaa95d5"
      },
      "source": [
        "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF\n",
        "\n",
        "# Remove stop words.\n",
        "wrangled = StopWordsRemover(inputCol='words', outputCol='terms')\\\n",
        "      .transform(wrangled)\n",
        "\n",
        "# Apply the hashing trick\n",
        "wrangled = HashingTF(inputCol='terms', outputCol='hash', numFeatures=1024)\\\n",
        "      .transform(wrangled)\n",
        "\n",
        "# Convert hashed symbols to TF-IDF\n",
        "tf_idf = IDF(inputCol='hash', outputCol='features')\\\n",
        "      .fit(wrangled).transform(wrangled)\n",
        "      \n",
        "tf_idf.select('terms', 'features').show(4, truncate=False)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "|terms                           |features                                                                                            |\n",
            "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "|[sorry, call, later, meeting]   |(1024,[138,384,577,996],[2.273418200008753,3.6288353225642043,3.5890949939146903,4.104259019279279])|\n",
            "|[dont, worry, guess, busy]      |(1024,[215,233,276,329],[3.9913186080986836,3.3790235241678332,4.734227298217693,4.58299632849377]) |\n",
            "|[call, freephone]               |(1024,[133,138],[5.367951058306837,2.273418200008753])                                              |\n",
            "|[win, cash, prize, prize, worth]|(1024,[31,47,62,389],[3.6632029660684124,4.754846585420428,4.072170704727778,7.064594791043114])    |\n",
            "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkQgJAIhR3eN"
      },
      "source": [
        "# Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNQAUCpFRMSZ",
        "outputId": "5ae6a65b-e807-49b8-cc08-99ed4f493756"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "# Split the data into training and testing sets\n",
        "sms_train, sms_test = tf_idf.randomSplit([0.8, 0.2], seed=13)\n",
        "\n",
        "# Fit a Logistic Regression model to the training data\n",
        "logistic = LogisticRegression(regParam=0.2).fit(sms_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "prediction = logistic.transform(sms_test)\n",
        "\n",
        "# Create a confusion matrix, comparing predictions to known labels\n",
        "prediction.groupBy('label', 'prediction').count().show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0|   41|\n",
            "|    0|       0.0|  948|\n",
            "|    1|       1.0|  105|\n",
            "|    0|       1.0|    2|\n",
            "+-----+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NE6ZO6wbSXOy",
        "outputId": "9a97bece-08fa-4939-972d-b285af63c43a"
      },
      "source": [
        "# Calculate the elements of the confusion matrix\n",
        "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
        "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
        "FN = prediction.filter('prediction = 0 AND label != prediction').count()\n",
        "FP = prediction.filter('prediction = 1 AND label != prediction').count()\n",
        "\n",
        "# Accuracy measures the proportion of correct predictions\n",
        "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
        "print(accuracy)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9607664233576643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG8TOh_XTB4F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}